{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.599618Z","iopub.status.busy":"2023-05-08T09:41:35.599182Z","iopub.status.idle":"2023-05-08T09:41:35.613564Z","shell.execute_reply":"2023-05-08T09:41:35.612112Z","shell.execute_reply.started":"2023-05-08T09:41:35.599588Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.utils import save_img\n","from matplotlib import pyplot as plt\n","import tensorflow_probability as tfp\n","from keras import backend as K\n","\n","from os import mkdir, path\n","import tensorflow as tf\n","import numpy as np\n","import pathlib\n","import glob\n","import cv2\n","\n","\n","DATASET_PATH = 'matlab_script/tasks/task_1/hk_test/synthetic_norm_irises/'\n","MODELS_PATH = 'task_3/feature_extractor/models/'\n","GENERATED_IMAGES_PATH = 'task_3/feature_extractor/models/'\n","\n","models_list = ['best_pix2pix_bs4_dice_loss_LP_5.h5', \n","               'best_unet_bs4_iou_loss_ep_80_estop_10']\n","\n","IMG_WIDTH = 240\n","IMG_HEIGHT = 64"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["testset_files = glob.glob(DATASET_PATH + '/*.png')\n","testset_files.sort()\n","PATHLIB_DATASET_PATH  = pathlib.Path(DATASET_PATH)\n","LENGTH_IMAGE_PATH = len(testset_files)\n","print(testset_files[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.617189Z","iopub.status.busy":"2023-05-08T09:41:35.616061Z","iopub.status.idle":"2023-05-08T09:41:35.623075Z","shell.execute_reply":"2023-05-08T09:41:35.622065Z","shell.execute_reply.started":"2023-05-08T09:41:35.617151Z"},"trusted":true},"outputs":[],"source":["def load(image_file):\n","    # Read and decode an image file to a uint8 tensor\n","    image = tf.io.read_file(image_file)\n","    image = tf.io.decode_png(image)\n","    image = tf.cast(image, tf.float32)\n","        \n","    return image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.625742Z","iopub.status.busy":"2023-05-08T09:41:35.624799Z","iopub.status.idle":"2023-05-08T09:41:35.640980Z","shell.execute_reply":"2023-05-08T09:41:35.639654Z","shell.execute_reply.started":"2023-05-08T09:41:35.625706Z"},"trusted":true},"outputs":[],"source":["def resize(input_image, height, width):\n","    input_image = tf.image.resize(input_image, [height, width],\n","                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","\n","    \n","    return input_image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.644089Z","iopub.status.busy":"2023-05-08T09:41:35.643563Z","iopub.status.idle":"2023-05-08T09:41:35.653190Z","shell.execute_reply":"2023-05-08T09:41:35.652306Z","shell.execute_reply.started":"2023-05-08T09:41:35.644055Z"},"trusted":true},"outputs":[],"source":["# Normalizing the images to [-1, 1]\n","def normalize(input_image):\n","    input_image = (input_image / 127.5) - 1\n","    return input_image"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# daugman feature extraction \n","\n","def tf_ProcessSingleChannel(channel):\n","    h = tf.histogram_fixed_width(channel, value_range=(0, 255), nbins=256)\n","\n","    h = tf.cast(h, tf.float32)\n","    pixel_values = tf.range(256, dtype=tf.float32)\n","    \n","    weighted_sum = tf.reduce_sum(pixel_values * h)\n","    total_pixels = tf.reduce_sum(h)\n","    mean_val = weighted_sum / total_pixels\n","\n","    # Compute variance and standard deviation\n","    variance = tf.reduce_sum(((pixel_values - mean_val) ** 2) * h) / total_pixels\n","    std_dev = tf.sqrt(variance)\n","\n","    # Compute Gaussian values\n","    gaussian_vals = (1 / (std_dev * tf.sqrt(2 * np.pi))) * tf.exp(-0.5 * ((pixel_values - mean_val) / std_dev) ** 2)\n","\n","    # Set threshold\n","    threshold = tf.reduce_max(gaussian_vals) * 0.1  # For example, 10% of the maximum\n","\n","    # Find values to eliminate\n","    to_eliminate = gaussian_vals < threshold\n","\n","    ProcessedChannel = tf.identity(channel)  # Create a copy\n","\n","    # Replace values below the threshold\n","    for i in range(len(to_eliminate)):\n","        if to_eliminate[i]:\n","            ProcessedChannel = tf.where(channel == i, mean_val + std_dev, ProcessedChannel)\n","\n","    return ProcessedChannel\n","\n","def tf_GaussHistCut(image):\n","    channels = 1\n","    if len(image.shape) > 2:\n","        _, _, channels = image.shape\n","\n","    if channels == 3:  # RGB image\n","        CorrectedImage = tf.zeros_like(image, dtype=tf.uint8)\n","\n","        for ch in range(channels):\n","            CorrectedImage[:, :, ch] = tf_ProcessSingleChannel(image[:, :, ch])\n","    \n","    else:  # Grayscale image\n","        CorrectedImage = tf_ProcessSingleChannel(image)\n","\n","    return CorrectedImage\n","\n","def tf_rescale(data):\n","    data_min = tf.reduce_min(data)\n","    data_max = tf.reduce_max(data)\n","    return (data - data_min) / (data_max - data_min)\n","\n","def tf_mad_normalize(channel):\n","    mad = tfp.stats.percentile(tf.abs(channel - tfp.stats.percentile(channel, 50)), 50)\n","    is_zero_mad = tf.equal(mad, 0)\n","    channel = tf.where(is_zero_mad, tf.zeros_like(channel), (channel - tfp.stats.percentile(channel, 50)) / mad)\n","    return tf_rescale(channel)\n","\n","def tf_daugman_normalization(AR) : #(image):\n","\n","    #AR, AG, AB = tf.split(image, num_or_size_splits=3, axis=-1)\n","\n","    # Apply GaussHistCut\n","    AR = tf_GaussHistCut(AR)\n","    #AG = tf_GaussHistCut(AG)\n","    #AB = tf_GaussHistCut(AB)\n","\n","    AR = tf_mad_normalize(AR)\n","    #AG = tf_mad_normalize(AG)\n","    #AB = tf_mad_normalize(AB)\n","\n","    # Replace NaN and Inf values with 0\n","    AR = tf.where(tf.math.is_nan(AR) | tf.math.is_inf(AR), tf.zeros_like(AR), AR)\n","    #AG = tf.where(tf.math.is_nan(AG) | tf.math.is_inf(AG), tf.zeros_like(AG), AG)\n","    #AB = tf.where(tf.math.is_nan(AB) | tf.math.is_inf(AB), tf.zeros_like(AB), AB)\n","\n","    # Create the normalized image\n","    #norm_image = tf.concat([AR, AG, AB], axis=-1)\n","\n","    return AR #return norm_image\n","    \n","def tf_gaborconvolve(im, nscale, minWaveLength, mult, sigmaOnf):\n","    rows = IMG_HEIGHT #im.shape[0]\n","    cols = IMG_WIDTH #im.shape[1]\n","    \n","    filtersum = tf.zeros(cols, dtype=tf.float32)\n","    EO = [None] * nscale\n","    \n","    ndata = cols\n","\n","    logGabor = tf.zeros(ndata, dtype=tf.float32)\n","    result = tf.zeros([rows, ndata], dtype=tf.complex128)\n","    \n","    radius = tf.range(0, ndata // 2 + 1, dtype=tf.float64) / (ndata // 2) / 2  # Frequency values 0 - 0.5\n","    zerovalue = tf.cast(tf.constant([1.0]), dtype=tf.float64)\n","    radius = tf.tensor_scatter_nd_update(radius, tf.constant([[0]]), zerovalue)\n","    \n","    wavelength = minWaveLength  # Initialize filter wavelength\n","    \n","    for s in range(nscale):\n","        # Construct the filter - first calculate the radial filter component\n","        fo = 1.0 / wavelength  # Centre frequency of filter\n","        # corresponding to fo\n","        \n","        sum = tf.exp( tf.cast( - tf.pow((tf.math.log(radius/fo)), 2), dtype=tf.float32) / (2 * tf.pow(tf.math.log(sigmaOnf), 2)))\n","\n","\n","        indexes = tf.expand_dims(tf.range(0, sum.shape[0]), axis=1)\n","\n","        logGabor = tf.tensor_scatter_nd_update(logGabor, indexes, sum)\n","        logGabor = tf.tensor_scatter_nd_update(logGabor, tf.constant([[0]]), tf.constant([0.0]))\n","        \n","        filter = logGabor\n","        filtersum = filtersum + filter\n","        \n","        for r in range(rows):\n","            signal = im[r, 0:ndata]\n","            imagefft = tf.signal.fft(tf.cast(signal, dtype=tf.complex128))\n","            filter = tf.cast(filter, dtype=tf.complex128)\n","            result = tf.tensor_scatter_nd_add(result, [tf.constant([r])], [tf.signal.ifft(imagefft * filter)])\n","        \n","        EO[s] = result\n","        wavelength *= mult  # Finally calculate the wavelength of the next filter\n","    \n","    filtersum = tf.signal.fftshift(filtersum)\n","    \n","    return EO, filtersum\n","\n","def tf_encode(polar_array, nscales, minWaveLength, mult, sigmaOnf):\n","    # Convoluzione della regione normalizzata con filtri di Gabor\n","    E0, _ = tf_gaborconvolve(polar_array, nscales, minWaveLength, mult, sigmaOnf)\n","    \n","    H = tf.zeros(E0[0].shape)\n","    for k in range(1, nscales + 1):\n","        E1 = E0[k - 1]\n","\n","        cond_0 = tf.math.logical_and(tf.math.real(E1) <= 0, tf.math.imag(E1) <= 0)\n","        cond_1 = tf.math.logical_and(tf.math.real(E1) <= 0, tf.math.imag(E1) > 0)\n","        cond_2 = tf.math.logical_and(tf.math.real(E1) > 0, tf.math.imag(E1) <= 0)\n","        cond_3 = tf.math.logical_and(tf.math.real(E1) > 0, tf.math.imag(E1) > 0)\n","\n","        H=tf.where(cond_0,0.0,H)\n","        H=tf.where(cond_1,1.0,H)\n","        H=tf.where(cond_2,2.0,H)\n","        H=tf.where(cond_3,3.0,H)\n","\n","    return H\n","\n","def tf_GaborBitStreamSTACKED(AR): #polarImage):\n","\n","    #AR, AG, AB = tf.split(polarImage, num_or_size_splits=3, axis=-1)\n","\n","    nscales = 1\n","    minWaveLength = 24\n","    mult = 1\n","    sigmaOnf = 0.5\n","\n","    TR = tf_encode(tf.squeeze(AR), nscales, minWaveLength, mult, sigmaOnf)\n","    #TG = tf_encode(tf.squeeze(AG), nscales, minWaveLength, mult, sigmaOnf)\n","    #TB = tf_encode(tf.squeeze(AB), nscales, minWaveLength, mult, sigmaOnf)\n","\n","    TR = tf.cast(TR, dtype=tf.uint8)\n","\n","    return tf.expand_dims(TR, axis=2) #return tf.concat([tf.expand_dims(TR, axis=2) , tf.expand_dims(TG, axis=2), tf.expand_dims(TB, axis=2)], axis=-1)\n","\n","def tf_daugman_feature_extractor(inp):\n","    return tf_GaborBitStreamSTACKED(inp)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import scipy.io\n","import os \n","\n","def save_image(image, images_path, i, grayscale):\n","\n","    parts = testset_files[i].split('/')\n","    file_name = parts[-1]\n","\n","    if grayscale==True:\n","        color_images_path = images_path + '/grayscale'\n","    else :\n","        color_images_path = images_path + '/rgb'\n","\n","    if not os.path.exists(color_images_path):\n","        os.makedirs(color_images_path)\n","\n","    file_path = color_images_path + '/' + file_name \n","\n","    file_path = file_path[0:(len(file_path)-4)]\n","    file_path = file_path + '.mat'\n","\n","    img = image[0].numpy()\n","    scipy.io.savemat(file_path, {'matrix': img})\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# printing img testset \n","\n","inp = load(testset_files[0])\n","ar_inp,_,_ = tf.split(inp, num_or_size_splits=3, axis=-1)\n","norm_ar = tf_daugman_normalization(ar_inp)\n","\n","plt.figure(figsize=(6, 6))\n","\n","display_list = [norm_ar]\n","title = ['Ground Truth']\n","\n","for i in range(1):\n","    plt.subplot(1, 2, i+1)\n","    plt.title(title[i])\n","    plt.imshow(display_list[i])\n","    plt.axis('off')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.667763Z","iopub.status.busy":"2023-05-08T09:41:35.667426Z","iopub.status.idle":"2023-05-08T09:41:35.678595Z","shell.execute_reply":"2023-05-08T09:41:35.677384Z","shell.execute_reply.started":"2023-05-08T09:41:35.667733Z"},"trusted":true},"outputs":[],"source":["def load_image_test(image_file):\n","    input_image = load(image_file)\n","    R,G,B = tf.split(input_image, num_or_size_splits=3, axis=-1)\n","    normR = tf_daugman_normalization(R)\n","    normG = tf_daugman_normalization(G)\n","    normB = tf_daugman_normalization(B)\n","    \n","    return normR, normG, normB"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.680557Z","iopub.status.busy":"2023-05-08T09:41:35.679799Z","iopub.status.idle":"2023-05-08T09:41:35.948344Z","shell.execute_reply":"2023-05-08T09:41:35.947016Z","shell.execute_reply.started":"2023-05-08T09:41:35.680526Z"},"trusted":true},"outputs":[],"source":["test_dataset = tf.data.Dataset.from_tensor_slices(testset_files)\n","test_dataset = test_dataset.map(load_image_test)\n","test_dataset = test_dataset.batch(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:40.733975Z","iopub.status.busy":"2023-05-08T09:41:40.733521Z","iopub.status.idle":"2023-05-08T09:43:42.460664Z","shell.execute_reply":"2023-05-08T09:43:42.459393Z","shell.execute_reply.started":"2023-05-08T09:41:40.733935Z"},"trusted":true},"outputs":[],"source":["for model_name in models_list :\n","    generator = tf.keras.models.load_model(MODELS_PATH + model_name + '.h5', compile=False)\n","    model_images_path = GENERATED_IMAGES_PATH + model_name\n","    mkdir(model_images_path)\n","\n","   #graycale \n","    n_image = 0\n","    for inpR, inpG, inpB in test_dataset :\n","        new_img = (inpR + inpG + inpB) / 3\n","        gen_output = generator(new_img, training=False)          \n","        save_image(gen_output, model_images_path, n_image, True)\n","        n_image = n_image + 1\n","\n","    #rgb \n","    n_image = 0\n","    for inpR, inpG, inpB in test_dataset :\n","        gen_outR = generator(inpR, training=False)    \n","        gen_outG = generator(inpG, training=False)    \n","        gen_outB = generator(inpB, training=False)    \n","        gen_output = tf.concat([gen_outR, gen_outG, gen_outB], axis=-1)\n","    \n","        save_image(gen_output, model_images_path, n_image, False)\n","        n_image = n_image + 1\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
