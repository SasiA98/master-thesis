{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.599618Z","iopub.status.busy":"2023-05-08T09:41:35.599182Z","iopub.status.idle":"2023-05-08T09:41:35.613564Z","shell.execute_reply":"2023-05-08T09:41:35.612112Z","shell.execute_reply.started":"2023-05-08T09:41:35.599588Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.utils import save_img\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from matplotlib import pyplot as plt\n","import tensorflow_probability as tfp\n","from keras import backend as K\n","from os import mkdir, path\n","import tensorflow as tf\n","import numpy as np\n","import cv2\n","import scipy.io\n","import os\n","import pathlib\n","import glob\n","\n","\n","hk_validation = True\n","model_selection = False\n","\n","DATASET_PATH = 'task_2/dataset/processed_hk_norm_unenhanced_aug_iris_dataset_64x240_png/'\n","\n","MODELS_PATH = 'task_2/transcoding/models/'\n","GENERATED_IMAGES_PATH = 'task2/transcoding/results/hk_dataset/'\n","\n","models_list = ['best_unet_bs4_LF_0.5_diceloss', \n","               'best_pix2pix_bs4_LF_1_diceloss']\n","\n","\n","IMG_WIDTH = 240\n","IMG_HEIGHT = 64"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["if hk_validation : \n","    # ---- ridurre la dimensione del testsize se ho usato anche il validation ---- #\n","    testset_files = glob.glob(DATASET_PATH + 'test/*.png')\n","    testset_files.sort()\n","\n","    new_test_files = []\n","    len_file = len(testset_files[1])\n","\n","    i=0\n","    for file in testset_files:\n","        parts = testset_files[i].split('/')\n","        file_name = parts[-1]\n","        sub = file_name[0:3]\n","        \n","        if model_selection :\n","            if int(sub) <= 180 :\n","                new_test_files.append(file)\n","        else : \n","            if int(sub) > 180 :\n","                new_test_files.append(file)\n","        i = i+1 \n","        \n","    testset_files = new_test_files\n","    testset_size = len(testset_files)\n","\n","else :\n","    \n","    testset_files = glob.glob(DATASET_PATH + 'test/*.png')\n","    testset_files.sort()\n","    testset_size = len(testset_files)\n","\n","\n","PATHLIB_DATASET_PATH  = pathlib.Path(DATASET_PATH)\n","LENGTH_IMAGE_PATH = len(testset_files[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.617189Z","iopub.status.busy":"2023-05-08T09:41:35.616061Z","iopub.status.idle":"2023-05-08T09:41:35.623075Z","shell.execute_reply":"2023-05-08T09:41:35.622065Z","shell.execute_reply.started":"2023-05-08T09:41:35.617151Z"},"trusted":true},"outputs":[],"source":["def load(image_file):\n","    # Read and decode an image file to a uint8 tensor\n","    image = tf.io.read_file(image_file)\n","\n","    image = tf.io.decode_png(image)\n","\n","    # Split each image tensor into two tensors:\n","    # - one with a real building facade image\n","    # - one with an architecture label image \n","    w = tf.shape(image)[1]\n","    w = w // 2\n","\n","    input_image = image[:, :w, :]\n","    real_image = image[:, w:, :]\n","\n","    # Convert both images to float32 tensors\n","    input_image = tf.cast(input_image, tf.float32)\n","    real_image = tf.cast(real_image, tf.float32)\n","        \n","    return input_image, real_image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.625742Z","iopub.status.busy":"2023-05-08T09:41:35.624799Z","iopub.status.idle":"2023-05-08T09:41:35.640980Z","shell.execute_reply":"2023-05-08T09:41:35.639654Z","shell.execute_reply.started":"2023-05-08T09:41:35.625706Z"},"trusted":true},"outputs":[],"source":["def resize(input_image, real_image, height, width):\n","    input_image = tf.image.resize(input_image, [height, width],\n","                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","    \n","    if dataset == 'hk_dataset' :\n","        real_image = tf.image.resize(real_image, [height, width],\n","                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","    \n","    return input_image, real_image"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Daugman feature extraction algorithm\n","\n","def tf_ProcessSingleChannel(channel):\n","    h = tf.histogram_fixed_width(channel, value_range=(0, 255), nbins=256)\n","\n","    h = tf.cast(h, tf.float32)\n","    pixel_values = tf.range(256, dtype=tf.float32)\n","    \n","    weighted_sum = tf.reduce_sum(pixel_values * h)\n","    total_pixels = tf.reduce_sum(h)\n","    mean_val = weighted_sum / total_pixels\n","\n","    # Compute variance and standard deviation\n","    variance = tf.reduce_sum(((pixel_values - mean_val) ** 2) * h) / total_pixels\n","    std_dev = tf.sqrt(variance)\n","\n","    # Compute Gaussian values\n","    gaussian_vals = (1 / (std_dev * tf.sqrt(2 * np.pi))) * tf.exp(-0.5 * ((pixel_values - mean_val) / std_dev) ** 2)\n","\n","    # Set threshold\n","    threshold = tf.reduce_max(gaussian_vals) * 0.1  # For example, 10% of the maximum\n","\n","    # Find values to eliminate\n","    to_eliminate = gaussian_vals < threshold\n","\n","    ProcessedChannel = tf.identity(channel)  # Create a copy\n","\n","    # Replace values below the threshold\n","    for i in range(len(to_eliminate)):\n","        if to_eliminate[i]:\n","            ProcessedChannel = tf.where(channel == i, mean_val + std_dev, ProcessedChannel)\n","\n","    return ProcessedChannel\n","\n","def tf_GaussHistCut(image):\n","    channels = 1\n","    if len(image.shape) > 2:\n","        _, _, channels = image.shape\n","\n","    if channels == 3:  # RGB image\n","        CorrectedImage = tf.zeros_like(image, dtype=tf.uint8)\n","\n","        for ch in range(channels):\n","            CorrectedImage[:, :, ch] = tf_ProcessSingleChannel(image[:, :, ch])\n","    \n","    else:  # Grayscale image\n","        CorrectedImage = tf_ProcessSingleChannel(image)\n","\n","    return CorrectedImage\n","\n","def tf_rescale(data):\n","    data_min = tf.reduce_min(data)\n","    data_max = tf.reduce_max(data)\n","    return (data - data_min) / (data_max - data_min)\n","\n","def tf_mad_normalize(channel):\n","    mad = tfp.stats.percentile(tf.abs(channel - tfp.stats.percentile(channel, 50)), 50)\n","    is_zero_mad = tf.equal(mad, 0)\n","    channel = tf.where(is_zero_mad, tf.zeros_like(channel), (channel - tfp.stats.percentile(channel, 50)) / mad)\n","    return tf_rescale(channel)\n","\n","def tf_daugman_normalization(AR) : #(image):\n","\n","    #AR, AG, AB = tf.split(image, num_or_size_splits=3, axis=-1)\n","\n","    # Apply GaussHistCut\n","    AR = tf_GaussHistCut(AR)\n","    #AG = tf_GaussHistCut(AG)\n","    #AB = tf_GaussHistCut(AB)\n","\n","    AR = tf_mad_normalize(AR)\n","    #AG = tf_mad_normalize(AG)\n","    #AB = tf_mad_normalize(AB)\n","\n","    # Replace NaN and Inf values with 0\n","    AR = tf.where(tf.math.is_nan(AR) | tf.math.is_inf(AR), tf.zeros_like(AR), AR)\n","    #AG = tf.where(tf.math.is_nan(AG) | tf.math.is_inf(AG), tf.zeros_like(AG), AG)\n","    #AB = tf.where(tf.math.is_nan(AB) | tf.math.is_inf(AB), tf.zeros_like(AB), AB)\n","\n","    # Create the normalized image\n","    #norm_image = tf.concat([AR, AG, AB], axis=-1)\n","\n","    return AR #return norm_image\n","    \n","def tf_gaborconvolve(im, nscale, minWaveLength, mult, sigmaOnf):\n","    rows = IMG_HEIGHT #im.shape[0]\n","    cols = IMG_WIDTH #im.shape[1]\n","    \n","    filtersum = tf.zeros(cols, dtype=tf.float32)\n","    EO = [None] * nscale\n","    \n","    ndata = cols\n","\n","    logGabor = tf.zeros(ndata, dtype=tf.float32)\n","    result = tf.zeros([rows, ndata], dtype=tf.complex128)\n","    \n","    radius = tf.range(0, ndata // 2 + 1, dtype=tf.float64) / (ndata // 2) / 2  # Frequency values 0 - 0.5\n","    zerovalue = tf.cast(tf.constant([1.0]), dtype=tf.float64)\n","    radius = tf.tensor_scatter_nd_update(radius, tf.constant([[0]]), zerovalue)\n","    \n","    wavelength = minWaveLength  # Initialize filter wavelength\n","    \n","    for s in range(nscale):\n","        # Construct the filter - first calculate the radial filter component\n","        fo = 1.0 / wavelength  # Centre frequency of filter\n","        # corresponding to fo\n","        \n","        sum = tf.exp( tf.cast( - tf.pow((tf.math.log(radius/fo)), 2), dtype=tf.float32) / (2 * tf.pow(tf.math.log(sigmaOnf), 2)))\n","\n","\n","        indexes = tf.expand_dims(tf.range(0, sum.shape[0]), axis=1)\n","\n","        logGabor = tf.tensor_scatter_nd_update(logGabor, indexes, sum)\n","        logGabor = tf.tensor_scatter_nd_update(logGabor, tf.constant([[0]]), tf.constant([0.0]))\n","        \n","        filter = logGabor\n","        filtersum = filtersum + filter\n","        \n","        for r in range(rows):\n","            signal = im[r, 0:ndata]\n","            imagefft = tf.signal.fft(tf.cast(signal, dtype=tf.complex128))\n","            filter = tf.cast(filter, dtype=tf.complex128)\n","            result = tf.tensor_scatter_nd_add(result, [tf.constant([r])], [tf.signal.ifft(imagefft * filter)])\n","        \n","        EO[s] = result\n","        wavelength *= mult  # Finally calculate the wavelength of the next filter\n","    \n","    filtersum = tf.signal.fftshift(filtersum)\n","    \n","    return EO, filtersum\n","\n","def tf_encode(polar_array, nscales, minWaveLength, mult, sigmaOnf):\n","    # Convoluzione della regione normalizzata con filtri di Gabor\n","    E0, _ = tf_gaborconvolve(polar_array, nscales, minWaveLength, mult, sigmaOnf)\n","    \n","    H = tf.zeros(E0[0].shape)\n","    for k in range(1, nscales + 1):\n","        E1 = E0[k - 1]\n","\n","        cond_0 = tf.math.logical_and(tf.math.real(E1) <= 0, tf.math.imag(E1) <= 0)\n","        cond_1 = tf.math.logical_and(tf.math.real(E1) <= 0, tf.math.imag(E1) > 0)\n","        cond_2 = tf.math.logical_and(tf.math.real(E1) > 0, tf.math.imag(E1) <= 0)\n","        cond_3 = tf.math.logical_and(tf.math.real(E1) > 0, tf.math.imag(E1) > 0)\n","\n","        H=tf.where(cond_0,0.0,H)\n","        H=tf.where(cond_1,1.0,H)\n","        H=tf.where(cond_2,2.0,H)\n","        H=tf.where(cond_3,3.0,H)\n","\n","    return H\n","\n","def tf_GaborBitStreamSTACKED(AR): #polarImage):\n","\n","    #AR, AG, AB = tf.split(polarImage, num_or_size_splits=3, axis=-1)\n","\n","    nscales = 1\n","    minWaveLength = 24\n","    mult = 1\n","    sigmaOnf = 0.5\n","\n","    TR = tf_encode(tf.squeeze(AR), nscales, minWaveLength, mult, sigmaOnf)\n","    #TG = tf_encode(tf.squeeze(AG), nscales, minWaveLength, mult, sigmaOnf)\n","    #TB = tf_encode(tf.squeeze(AB), nscales, minWaveLength, mult, sigmaOnf)\n","\n","    TR = tf.cast(TR, dtype=tf.uint8)\n","\n","    return tf.expand_dims(TR, axis=2) #return tf.concat([tf.expand_dims(TR, axis=2) , tf.expand_dims(TG, axis=2), tf.expand_dims(TB, axis=2)], axis=-1)\n","\n","def tf_daugman_feature_extractor(inp):\n","    return tf_GaborBitStreamSTACKED(inp)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.644089Z","iopub.status.busy":"2023-05-08T09:41:35.643563Z","iopub.status.idle":"2023-05-08T09:41:35.653190Z","shell.execute_reply":"2023-05-08T09:41:35.652306Z","shell.execute_reply.started":"2023-05-08T09:41:35.644055Z"},"trusted":true},"outputs":[],"source":["# Normalizing the images to [-1, 1]\n","def normalize(input_image, real_image):\n","    input_image = (input_image / 127.5) - 1\n","    real_image = (real_image / 127.5) - 1\n","         \n","    return input_image, real_image\n","\n","def normalize2(input_image, real_image) :\n","    \n","    mean = tf.math.reduce_mean(input_image)\n","    std = tf.math.reduce_std(input_image)\n","    input_image = tf.clip_by_value((input_image - mean) / std, clip_value_min=-1.0, clip_value_max=1.0)\n","\n","    mean = tf.math.reduce_mean(real_image)\n","    std = tf.math.reduce_std(real_image)\n","    real_image = tf.clip_by_value((real_image - mean) / std, clip_value_min=-1.0, clip_value_max=1.0)\n","    \n","    return input_image, real_image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.655031Z","iopub.status.busy":"2023-05-08T09:41:35.654382Z","iopub.status.idle":"2023-05-08T09:41:35.665492Z","shell.execute_reply":"2023-05-08T09:41:35.664187Z","shell.execute_reply.started":"2023-05-08T09:41:35.655000Z"},"trusted":true},"outputs":[],"source":["def generate_images(model, inp, tar):\n","    prediction = model(inp, training=False)\n","    plt.figure(figsize=(15, 15))\n","\n","    display_list = [tar[0], prediction[0]]\n","    title = ['Ground Truth', 'Predicted Image']\n","\n","    for i in range(2):\n","        plt.subplot(1, 2, i+1)\n","        plt.title(title[i])\n","        # Getting the pixel values in the [0, 1] range to plot.\n","        plt.imshow(display_list[i] * 0.5 + 0.5)\n","        plt.axis('off')\n","    plt.show()\n","\n","def print_images(gen_output, inp):\n","\n","    plt.figure(figsize=(15, 15))\n","    display_list = [inp[0], gen_output[0]]\n","    title = ['Ground Truth', 'Predicted Image']\n","    \n","    for i in range(2):\n","        plt.subplot(1, 2, i+1)\n","        plt.title(title[i])\n","        # Getting the pixel values in the [0, 1] range to plot.\n","        plt.imshow(display_list[i] * 0.5 + 0.5)\n","        plt.axis('off')\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def save_image(image, images_path, i):\n","    # Split the filename_stem using '/' as the delimiter\n","    parts = testset_files[i].split('/')\n","    file_name = parts[-1]\n","    png_images_path = images_path + '/png'\n","\n","    if not os.path.exists(png_images_path):\n","        os.makedirs(png_images_path)\n","\n","    file_path = png_images_path + '/' + file_name\n","    #print(file_path) \n","\n","    img = image[0].numpy()\n","    img = cv2.normalize(img, None, alpha = 0, beta = 255, norm_type = cv2.NORM_MINMAX, dtype = cv2.CV_32F)\n","    img = img.astype(np.uint8)\n","\n","    img = np.concatenate((img[..., np.newaxis],img[..., np.newaxis],img[..., np.newaxis]), axis=2)\n","    cv2.imwrite(file_path, img, [int(cv2.IMWRITE_PNG_COMPRESSION), 0])\n","\n","    matlab_images_path = images_path + '/mat'\n","\n","    if not os.path.exists(matlab_images_path):\n","        os.makedirs(matlab_images_path)\n","\n","    file_path = matlab_images_path + '/' + file_name \n","\n","    file_path = file_path[0:(len(file_path)-4)]\n","    file_path = file_path + '.mat'\n","\n","    img = tf_daugman_feature_extractor(image[0]) \n","\n","    scipy.io.savemat(file_path, {'matrix': img.numpy()})\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# printing img testset \n","\n","img_filename = testset_files[1]\n","print(img_filename)\n","inp, re = load(img_filename)\n","inp, re = resize(inp,re, IMG_HEIGHT, IMG_WIDTH)\n","inp, re = normalize(inp,re)\n","plt.figure(figsize=(6, 6))\n","\n","display_list = [inp]\n","title = ['Ground Truth']\n","\n","for i in range(1):\n","    plt.subplot(1, 2, i+1)\n","    plt.title(title[i])\n","    plt.imshow(display_list[i]* 0.5 + 0.5)\n","    plt.axis('off')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.667763Z","iopub.status.busy":"2023-05-08T09:41:35.667426Z","iopub.status.idle":"2023-05-08T09:41:35.678595Z","shell.execute_reply":"2023-05-08T09:41:35.677384Z","shell.execute_reply.started":"2023-05-08T09:41:35.667733Z"},"trusted":true},"outputs":[],"source":["def load_image_test(image_file):\n","    input_image, real_image = load(image_file)\n","    \n","    fi_real_image = []\n","\n","    norm_input_image, norm_real_image = normalize2(input_image, real_image)\n","    norm_input_image,_,_ = tf.split(norm_input_image, num_or_size_splits=3, axis=-1)\n","    norm_real_image,_,_ = tf.split(norm_real_image, num_or_size_splits=3, axis=-1)\n","\n","    fi_real_image = tf_daugman_feature_extractor(norm_real_image)\n","\n","    return norm_input_image, norm_real_image, fi_real_image\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.680557Z","iopub.status.busy":"2023-05-08T09:41:35.679799Z","iopub.status.idle":"2023-05-08T09:41:35.948344Z","shell.execute_reply":"2023-05-08T09:41:35.947016Z","shell.execute_reply.started":"2023-05-08T09:41:35.680526Z"},"trusted":true},"outputs":[],"source":["if hk_validation :\n","    test_dataset = tf.data.Dataset.from_tensor_slices(testset_files)\n","    test_dataset = test_dataset.map(load_image_test)\n","    test_dataset = test_dataset.batch(1)\n","else :\n","    test_dataset = tf.data.Dataset.list_files(str(PATHLIB_DATASET_PATH / 'test/*.png'), shuffle=False)\n","    test_dataset = test_dataset.map(load_image_test)\n","    test_dataset = test_dataset.batch(1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:40.724007Z","iopub.status.busy":"2023-05-08T09:41:40.723673Z","iopub.status.idle":"2023-05-08T09:41:40.729844Z","shell.execute_reply":"2023-05-08T09:41:40.728712Z","shell.execute_reply.started":"2023-05-08T09:41:40.723979Z"},"trusted":true},"outputs":[],"source":["def generator_loss(target, gen_output):\n","    # PSNR \n","    psnr_loss = tf.image.psnr(target, gen_output, max_val=2.0) # images shuld have been normalized in range [-1,1]. Thus, the difference between the min and max should be 2.\n","    \n","    # SSIM \n","    ssim_loss = tf.image.ssim(target, gen_output, max_val=2.0)\n","\n","    # Mean absolute error\n","    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n","\n","    # Mean squared error\n","    l2_loss = tf.reduce_mean(tf.abs(target - gen_output)**2)\n","    \n","    return psnr_loss.numpy().item(0), ssim_loss.numpy().item(0), l1_loss.numpy(), l2_loss.numpy()\n","\n","\n","def create_mask(img):\n","    pred_fi_image = tf_daugman_feature_extractor(img)\n","    pred_fi_image = pred_fi_image[tf.newaxis, ...]\n","    return pred_fi_image\n","\n","\n","def DiceLoss(targets, inputs, smooth=1e-6):\n","\n","    intersection = K.sum(targets * inputs)\n","    dice = (2*intersection + smooth) / (K.sum(targets) + K.sum(inputs) + smooth)\n","    return 1 - dice\n","\n","\n","def IoULoss(targets, inputs, smooth=1e-6):\n","\n","    intersection = K.sum(targets * inputs)\n","    total = K.sum(targets) + K.sum(inputs)\n","    union = total - intersection\n","    \n","    IoU = (intersection + smooth) / (union + smooth)\n","    return 1 - IoU\n","\n","\n","def fe_loss(y_true_m, mask_prediction_m) :\n","\n","    # OHE + flatten representation \n","\n","    y_true = K.squeeze(y_true_m, 3)\n","    y_true = tf.cast(y_true, \"int32\")\n","\n","    mask = K.squeeze(mask_prediction_m, 3)\n","    mask = tf.cast(mask, \"int32\")\n","\n","    y_true = K.flatten(y_true)\n","    mask = K.flatten(mask)\n","\n","    #accuracy   -   precision   -   recall   -   f1score\n","\n","    precision = precision_score(y_true, mask,  average='weighted')\n","    recall = recall_score(y_true, mask, average='weighted')\n","    f1 = f1_score(y_true, mask, average='weighted')\n","\n","\n","    # DICE + IOU \n","\n","    y_true_m = K.squeeze(y_true_m, 3)\n","    y_true_m = tf.one_hot(y_true_m, 4, axis=-1)\n","    y_true_m = tf.cast(y_true_m, \"float32\")\n","\n","    mask_prediction_m = K.squeeze(mask_prediction_m, 3)\n","    mask_prediction_m = tf.one_hot(mask_prediction_m, 4, axis=-1)  \n","    mask_prediction_m = tf.cast(mask_prediction_m, \"float32\")\n","\n","    #flatten label and prediction tensors\n","    y_true_m = K.flatten(y_true_m)\n","    mask_prediction_m = K.flatten(mask_prediction_m)\n","\n","    dice_loss = DiceLoss(y_true_m, mask_prediction_m)    \n","    iou_loss =  IoULoss(y_true_m, mask_prediction_m)\n","\n","    return dice_loss, iou_loss, precision, recall, f1"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:40.733975Z","iopub.status.busy":"2023-05-08T09:41:40.733521Z","iopub.status.idle":"2023-05-08T09:43:42.460664Z","shell.execute_reply":"2023-05-08T09:43:42.459393Z","shell.execute_reply.started":"2023-05-08T09:41:40.733935Z"},"trusted":true},"outputs":[],"source":["for model_name in models_list :\n","    generator = tf.keras.models.load_model(MODELS_PATH + model_name + '.h5', compile=False)\n","    model_images_path = GENERATED_IMAGES_PATH + model_name\n","\n","    mkdir(model_images_path)\n","\n","    sum_l1_losses = [] \n","    sum_l2_losses = []\n","    sum_psnr_losses = []\n","    sum_ssim_losses = []\n","    sum_acc = []\n","    sum_dice_loss = [] \n","    sum_iou_loss = [] \n","    sum_precision = []\n","    sum_recall = []\n","    sum_f1score = []\n","\n","    n_image = 0\n","    for inp, tar, tar_fi in test_dataset :\n","        gen_output = generator(inp, training=False)\n","        gen_fi = create_mask(gen_output)\n","        \n","        psnr_loss, ssim_loss, l1_loss, l2_loss = generator_loss(tar, gen_output)\n","\n","        sum_psnr_losses.append(psnr_loss)\n","        sum_ssim_losses.append(ssim_loss)\n","        sum_l1_losses.append(l1_loss)\n","        sum_l2_losses.append(l2_loss)\n","    \n","\n","        dice_loss, iou_loss, precision, recall, f1score = fe_loss(tar_fi, gen_fi)\n","\n","        sum_dice_loss.append(dice_loss)\n","        sum_iou_loss.append(iou_loss)\n","        sum_precision.append(precision)\n","        sum_recall.append(recall)\n","        sum_f1score.append(f1score)\n","                \n","        save_image(gen_output, model_images_path, n_image)\n","        n_image = n_image + 1\n","\n","    print('\\t' + model_name)\n","    print(\"\\t  PSNR loss\")\n","    print(\"\\t\\t  mean  : \" +  \"{:.2f}\".format(np.mean(sum_psnr_losses)) +  \"\\tvar : \" +  \"{:.2f}\".format(np.var(sum_psnr_losses)))\n","    print(\"\\t  SSIM loss\")\n","    print(\"\\t\\t  mean  : \" +  \"{:.2f}\".format(np.mean(sum_ssim_losses)) +  \"\\tvar : \" +  \"{:.2f}\".format(np.var(sum_ssim_losses)))\n","    print(\"\\t  L1 loss - L2 loss\")\n","    print(\"\\t\\t  l1_loss  : \" +  \"{:.2f}\".format(np.mean(sum_l1_losses)) +  \"\\tl2_loss : \" +  \"{:.2f}\".format(np.mean(sum_l2_losses)))\n","\n","    print(\"\\t  precision\")\n","    print(\"\\t\\t  mean  : \" +  \"{:.2f}\".format(np.mean(sum_precision)) +  \"\\tvar : \" +  \"{:.2f}\".format(np.var(sum_precision)))\n","    print(\"\\t  recall\")\n","    print(\"\\t\\t  mean  : \" +  \"{:.2f}\".format(np.mean(sum_recall)) +  \"\\tvar : \" +  \"{:.2f}\".format(np.var(sum_recall)))\n","    print(\"\\t  f1 score\")\n","    print(\"\\t\\t  mean  : \" +  \"{:.2f}\".format(np.mean(sum_f1score)) +  \"\\tvar : \" +  \"{:.2f}\".format(np.var(sum_f1score)))\n","    print(\"\\t  dice loss with label\")\n","    print(\"\\t\\t  mean  : \" +  \"{:.2f}\".format(np.mean(sum_dice_loss)) +  \"\\tvar : \" +  \"{:.2f}\".format(np.var(sum_dice_loss)))\n","    print(\"\\t  iou loss with label\")\n","    print(\"\\t\\t  mean  : \" +  \"{:.2f}\".format(np.mean(sum_iou_loss)) +  \"\\tvar : \" +  \"{:.2f}\".format(np.var(sum_iou_loss)))\n","    print(\"\\n\\n\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
