{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.599618Z","iopub.status.busy":"2023-05-08T09:41:35.599182Z","iopub.status.idle":"2023-05-08T09:41:35.613564Z","shell.execute_reply":"2023-05-08T09:41:35.612112Z","shell.execute_reply.started":"2023-05-08T09:41:35.599588Z"},"trusted":true},"outputs":[],"source":["from tensorflow.keras.utils import save_img\n","import tensorflow_probability as tfp\n","from matplotlib import pyplot as plt\n","from os import mkdir, path\n","from keras import backend as K\n","import tensorflow as tf\n","import numpy as np\n","import pathlib\n","import glob\n","import cv2\n","\n","\n","DATASET_PATH = 'task_2/dataset/processed_hk_norm_unenhanced_aug_iris_dataset_64x240_png/'\n","MODELS_PATH = 'task_2/cnn-based_dfe/models/'\n","\n","models_list = ['best_unet_bs4_dice_loss']\n","\n","IMG_WIDTH = 240\n","IMG_HEIGHT = 64\n","LENGTH_IMAGE_NAME = 12 \n","\n","# ---- ridurre la dimensione del testsize se ho usato anche il validation ---- #\n","testset_files = glob.glob(DATASET_PATH + 'test/*.png')\n","testset_files.sort()\n","\n","new_test_files = []\n","len_file = len(testset_files[1])\n","\n","i=0\n","for file in testset_files:\n","    parts = testset_files[i].split('/')\n","    file_name = parts[-1]\n","    sub = file_name[0:3]\n","    \n","    if int(sub) > 180 :\n","        new_test_files.append(file)\n","    i = i+1\n","    \n","testset_files = new_test_files\n","testset_size = len(testset_files)\n","\n","PATHLIB_DATASET_PATH  = pathlib.Path(DATASET_PATH)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.617189Z","iopub.status.busy":"2023-05-08T09:41:35.616061Z","iopub.status.idle":"2023-05-08T09:41:35.623075Z","shell.execute_reply":"2023-05-08T09:41:35.622065Z","shell.execute_reply.started":"2023-05-08T09:41:35.617151Z"},"trusted":true},"outputs":[],"source":["def load(image_file):\n","    # Read and decode an image file to a uint8 tensor\n","    image = tf.io.read_file(image_file)\n","\n","    image = tf.io.decode_png(image)\n","\n","    # Split each image tensor into two tensors:\n","    # - one with a real building facade image\n","    # - one with an architecture label image \n","    w = tf.shape(image)[1]\n","    w = w // 2\n","\n","    input_image = image[:, :w, :]\n","    real_image = image[:, w:, :]\n","\n","    # Convert both images to float32 tensors\n","    input_image = tf.cast(input_image, tf.float32)\n","    real_image = tf.cast(real_image, tf.float32)\n","        \n","    return input_image, real_image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.625742Z","iopub.status.busy":"2023-05-08T09:41:35.624799Z","iopub.status.idle":"2023-05-08T09:41:35.640980Z","shell.execute_reply":"2023-05-08T09:41:35.639654Z","shell.execute_reply.started":"2023-05-08T09:41:35.625706Z"},"trusted":true},"outputs":[],"source":["def resize(input_image, real_image, height, width):\n","    input_image = tf.image.resize(input_image, [height, width],\n","                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","    \n","    real_image = tf.image.resize(real_image, [height, width],\n","                        method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n","    \n","    return input_image, real_image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.644089Z","iopub.status.busy":"2023-05-08T09:41:35.643563Z","iopub.status.idle":"2023-05-08T09:41:35.653190Z","shell.execute_reply":"2023-05-08T09:41:35.652306Z","shell.execute_reply.started":"2023-05-08T09:41:35.644055Z"},"trusted":true},"outputs":[],"source":["# Normalizing the images to [-1, 1]\n","def normalize(input_image, real_image):\n","    input_image = (input_image / 127.5) - 1\n","    real_image = (real_image / 127.5) - 1\n","         \n","    return input_image, real_image\n","\n","def normalize2(input_image, real_image) :\n","    \n","    mean = tf.math.reduce_mean(input_image)\n","    std = tf.math.reduce_std(input_image)\n","    input_image = tf.clip_by_value((input_image - mean) / std, clip_value_min=-1.0, clip_value_max=1.0)\n","\n","    mean = tf.math.reduce_mean(real_image)\n","    std = tf.math.reduce_std(real_image)\n","    real_image = tf.clip_by_value((real_image - mean) / std, clip_value_min=-1.0, clip_value_max=1.0)\n","    \n","    return input_image, real_image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.655031Z","iopub.status.busy":"2023-05-08T09:41:35.654382Z","iopub.status.idle":"2023-05-08T09:41:35.665492Z","shell.execute_reply":"2023-05-08T09:41:35.664187Z","shell.execute_reply.started":"2023-05-08T09:41:35.655000Z"},"trusted":true},"outputs":[],"source":["def generate_images(model, test_input, tar):\n","    prediction = model(test_input, training=False)\n","    rbg_prediction = cv2.cvtColor(prediction[0].numpy(), cv2.COLOR_BGR2GRAY)\n","    rbg_tar = cv2.cvtColor(tar[0].numpy(), cv2.COLOR_BGR2GRAY)\n","    \n","    plt.figure(figsize=(15, 15))\n","\n","    display_list = [test_input[0], rbg_tar, rbg_prediction]\n","    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n","\n","    for i in range(3):\n","        plt.subplot(1, 3, i+1)\n","        plt.title(title[i])\n","        # Getting the pixel values in the [0, 1] range to plot.\n","        plt.imshow(display_list[i] * 0.5 + 0.5)\n","        plt.axis('off')\n","    \n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Daugman feature extraction algorithm\n","\n","def tf_ProcessSingleChannel(channel):\n","    h = tf.histogram_fixed_width(channel, value_range=(0, 255), nbins=256)\n","\n","    h = tf.cast(h, tf.float32)\n","    pixel_values = tf.range(256, dtype=tf.float32)\n","    \n","    weighted_sum = tf.reduce_sum(pixel_values * h)\n","    total_pixels = tf.reduce_sum(h)\n","    mean_val = weighted_sum / total_pixels\n","\n","    # Compute variance and standard deviation\n","    variance = tf.reduce_sum(((pixel_values - mean_val) ** 2) * h) / total_pixels\n","    std_dev = tf.sqrt(variance)\n","\n","    # Compute Gaussian values\n","    gaussian_vals = (1 / (std_dev * tf.sqrt(2 * np.pi))) * tf.exp(-0.5 * ((pixel_values - mean_val) / std_dev) ** 2)\n","\n","    # Set threshold\n","    threshold = tf.reduce_max(gaussian_vals) * 0.1  # For example, 10% of the maximum\n","\n","    # Find values to eliminate\n","    to_eliminate = gaussian_vals < threshold\n","\n","    ProcessedChannel = tf.identity(channel)  # Create a copy\n","\n","    # Replace values below the threshold\n","    for i in range(len(to_eliminate)):\n","        if to_eliminate[i]:\n","            ProcessedChannel = tf.where(channel == i, mean_val + std_dev, ProcessedChannel)\n","\n","    return ProcessedChannel\n","\n","def tf_GaussHistCut(image):\n","    channels = 1\n","    if len(image.shape) > 2:\n","        _, _, channels = image.shape\n","\n","    if channels == 3:  # RGB image\n","        CorrectedImage = tf.zeros_like(image, dtype=tf.uint8)\n","\n","        for ch in range(channels):\n","            CorrectedImage[:, :, ch] = tf_ProcessSingleChannel(image[:, :, ch])\n","    \n","    else:  # Grayscale image\n","        CorrectedImage = tf_ProcessSingleChannel(image)\n","\n","    return CorrectedImage\n","\n","def tf_rescale(data):\n","    data_min = tf.reduce_min(data)\n","    data_max = tf.reduce_max(data)\n","    return (data - data_min) / (data_max - data_min)\n","\n","def tf_mad_normalize(channel):\n","    mad = tfp.stats.percentile(tf.abs(channel - tfp.stats.percentile(channel, 50)), 50)\n","    is_zero_mad = tf.equal(mad, 0)\n","    channel = tf.where(is_zero_mad, tf.zeros_like(channel), (channel - tfp.stats.percentile(channel, 50)) / mad)\n","    return tf_rescale(channel)\n","\n","def tf_daugman_normalization(image):\n","\n","    AR, AG, AB = tf.split(image, num_or_size_splits=3, axis=-1)\n","\n","    # Apply GaussHistCut\n","    AR = tf_GaussHistCut(AR)\n","    #AG = tf_GaussHistCut(AG)\n","    #AB = tf_GaussHistCut(AB)\n","\n","    AR = tf_mad_normalize(AR)\n","    #AG = tf_mad_normalize(AG)\n","    #AB = tf_mad_normalize(AB)\n","\n","    # Replace NaN and Inf values with 0\n","    AR = tf.where(tf.math.is_nan(AR) | tf.math.is_inf(AR), tf.zeros_like(AR), AR)\n","    #AG = tf.where(tf.math.is_nan(AG) | tf.math.is_inf(AG), tf.zeros_like(AG), AG)\n","    #AB = tf.where(tf.math.is_nan(AB) | tf.math.is_inf(AB), tf.zeros_like(AB), AB)\n","\n","    # Create the normalized image\n","    #norm_image = tf.concat([AR, AG, AB], axis=-1)\n","\n","    return AR #return norm_image\n","    \n","def tf_gaborconvolve(im, nscale, minWaveLength, mult, sigmaOnf):\n","    rows = IMG_HEIGHT #im.shape[0]\n","    cols = IMG_WIDTH #im.shape[1]\n","    \n","    filtersum = tf.zeros(cols, dtype=tf.float32)\n","    EO = [None] * nscale\n","    \n","    ndata = cols\n","\n","    logGabor = tf.zeros(ndata, dtype=tf.float32)\n","    result = tf.zeros([rows, ndata], dtype=tf.complex128)\n","    \n","    radius = tf.range(0, ndata // 2 + 1, dtype=tf.float64) / (ndata // 2) / 2  # Frequency values 0 - 0.5\n","    zerovalue = tf.cast(tf.constant([1.0]), dtype=tf.float64)\n","    radius = tf.tensor_scatter_nd_update(radius, tf.constant([[0]]), zerovalue)\n","    \n","    wavelength = minWaveLength  # Initialize filter wavelength\n","    \n","    for s in range(nscale):\n","        # Construct the filter - first calculate the radial filter component\n","        fo = 1.0 / wavelength  # Centre frequency of filter\n","        # corresponding to fo\n","        \n","        sum = tf.exp( tf.cast( - tf.pow((tf.math.log(radius/fo)), 2), dtype=tf.float32) / (2 * tf.pow(tf.math.log(sigmaOnf), 2)))\n","\n","\n","        indexes = tf.expand_dims(tf.range(0, sum.shape[0]), axis=1)\n","\n","        logGabor = tf.tensor_scatter_nd_update(logGabor, indexes, sum)\n","        logGabor = tf.tensor_scatter_nd_update(logGabor, tf.constant([[0]]), tf.constant([0.0]))\n","        \n","        filter = logGabor\n","        filtersum = filtersum + filter\n","        \n","        for r in range(rows):\n","            signal = im[r, 0:ndata]\n","            imagefft = tf.signal.fft(tf.cast(signal, dtype=tf.complex128))\n","            filter = tf.cast(filter, dtype=tf.complex128)\n","            result = tf.tensor_scatter_nd_add(result, [tf.constant([r])], [tf.signal.ifft(imagefft * filter)])\n","        \n","        EO[s] = result\n","        wavelength *= mult  # Finally calculate the wavelength of the next filter\n","    \n","    filtersum = tf.signal.fftshift(filtersum)\n","    \n","    return EO, filtersum\n","\n","def tf_encode(polar_array, nscales, minWaveLength, mult, sigmaOnf):\n","    # Convoluzione della regione normalizzata con filtri di Gabor\n","    E0, _ = tf_gaborconvolve(polar_array, nscales, minWaveLength, mult, sigmaOnf)\n","    \n","    H = tf.zeros(E0[0].shape)\n","    for k in range(1, nscales + 1):\n","        E1 = E0[k - 1]\n","\n","        cond_0 = tf.math.logical_and(tf.math.real(E1) <= 0, tf.math.imag(E1) <= 0)\n","        cond_1 = tf.math.logical_and(tf.math.real(E1) <= 0, tf.math.imag(E1) > 0)\n","        cond_2 = tf.math.logical_and(tf.math.real(E1) > 0, tf.math.imag(E1) <= 0)\n","        cond_3 = tf.math.logical_and(tf.math.real(E1) > 0, tf.math.imag(E1) > 0)\n","\n","        H=tf.where(cond_0,0.0,H)\n","        H=tf.where(cond_1,1.0,H)\n","        H=tf.where(cond_2,2.0,H)\n","        H=tf.where(cond_3,3.0,H)\n","\n","    return H\n","\n","def tf_GaborBitStreamSTACKED(AR): #polarImage):\n","\n","    #AR, AG, AB = tf.split(polarImage, num_or_size_splits=3, axis=-1)\n","\n","    nscales = 1\n","    minWaveLength = 24\n","    mult = 1\n","    sigmaOnf = 0.5\n","\n","    TR = tf_encode(tf.squeeze(AR), nscales, minWaveLength, mult, sigmaOnf)\n","    #TG = tf_encode(tf.squeeze(AG), nscales, minWaveLength, mult, sigmaOnf)\n","    #TB = tf_encode(tf.squeeze(AB), nscales, minWaveLength, mult, sigmaOnf)\n","\n","    TR = tf.cast(TR, dtype=tf.uint8)\n","\n","    return tf.expand_dims(TR, axis=2) #return tf.concat([tf.expand_dims(TR, axis=2) , tf.expand_dims(TG, axis=2), tf.expand_dims(TB, axis=2)], axis=-1)\n","\n","def tf_daugman_feature_extractor(inp):\n","    return tf_GaborBitStreamSTACKED(inp)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.667763Z","iopub.status.busy":"2023-05-08T09:41:35.667426Z","iopub.status.idle":"2023-05-08T09:41:35.678595Z","shell.execute_reply":"2023-05-08T09:41:35.677384Z","shell.execute_reply.started":"2023-05-08T09:41:35.667733Z"},"trusted":true},"outputs":[],"source":["def load_image_test(image_file):\n","        \n","    input_image, real_image = load(image_file)\n","    _, norm_real_image = normalize2(input_image, real_image)\n","    \n","    norm_R, _, _ = tf.split(norm_real_image, num_or_size_splits=3, axis=-1)\n","\n","    fi_real_image = tf_daugman_feature_extractor(norm_R)\n","\n","    return norm_R, fi_real_image"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:35.680557Z","iopub.status.busy":"2023-05-08T09:41:35.679799Z","iopub.status.idle":"2023-05-08T09:41:35.948344Z","shell.execute_reply":"2023-05-08T09:41:35.947016Z","shell.execute_reply.started":"2023-05-08T09:41:35.680526Z"},"trusted":true},"outputs":[],"source":["test_dataset = tf.data.Dataset.from_tensor_slices(testset_files)\n","test_dataset = test_dataset.map(load_image_test)\n","test_dataset = test_dataset.batch(1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def generator_loss(target, gen_output):\n","    # PSNR \n","    psnr_loss = tf.image.psnr(target, gen_output, max_val=2.0) # images shuld have been normalized in range [-1,1]. Thus, the difference between the min and max should be 2.\n","    \n","    # SSIM \n","    ssim_loss = tf.image.ssim(target, gen_output, max_val=2.0)\n","\n","    # Mean absolute error\n","    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n","\n","    # Mean squared error\n","    l2_loss = tf.reduce_mean(tf.abs(target - gen_output)**2)\n","    \n","    return psnr_loss.numpy().item(0), ssim_loss.numpy().item(0), l1_loss.numpy(), l2_loss.numpy()\n","\n","\n","def create_mask(pred_mask):\n"," pred_mask = tf.argmax(pred_mask, axis=-1)\n"," pred_mask = pred_mask[..., tf.newaxis]\n"," return pred_mask\n","\n","def DiceLoss(targets, inputs, smooth=1e-6):\n","    # convert the tensor to one-hot for multi-class segmentation\n","    #y_true = K.squeeze(y_true, 3)\n","    #y_true = tf.cast(y_true, \"int32\")\n","    #y_true = tf.one_hot(y_true, 4, axis=-1)\n","    \n","    # cast to float32 datatype\n","    #y_true = K.cast(y_true, 'float32')\n","    #y_pred = K.cast(y_pred, 'float32')\n","    \n","    #flatten label and prediction tensors\n","    #inputs = K.flatten(y_pred)\n","    #targets = K.flatten(y_true)\n","\n","    intersection = K.sum(targets * inputs)\n","    dice = (2*intersection + smooth) / (K.sum(targets) + K.sum(inputs) + smooth)\n","    return 1 - dice\n","\n","\n","#Tensorflow / Keras \n","def IoULoss(targets, inputs, smooth=1e-6):\n","\n","\n","    intersection = K.sum(targets * inputs)\n","    total = K.sum(targets) + K.sum(inputs)\n","    union = total - intersection\n","    \n","    IoU = (intersection + smooth) / (union + smooth)\n","    return 1 - IoU\n","\n","\n","def segmentation_loss(y_true, y_pred) :\n","\n","    # convert the tensor to one-hot for multi-class segmentation\n","    mask_prediction = create_mask(y_pred)\n","    mask_prediction = tf.cast(mask_prediction, tf.uint8)\n","\n","    y_true_m = K.squeeze(y_true, 3)\n","    y_true_m = tf.cast(y_true_m, \"int32\")\n","    y_true_m1 = tf.one_hot(y_true_m, 4, axis=-1)\n","\n","    mask_prediction_m = K.squeeze(mask_prediction, 3)\n","    mask_prediction_m = tf.cast(mask_prediction_m, \"int32\")\n","    mask_prediction_m1 = tf.one_hot(mask_prediction_m, 4, axis=-1)  \n","    \n","    # cast to float32 datatype\n","    y_true_m = K.cast(y_true_m1, 'float32')\n","    y_pred_m = K.cast(y_pred, 'float32')\n","    mask_prediction_m =  K.cast(mask_prediction_m1, 'float32')\n","    \n","    #flatten label and prediction tensors\n","    y_pred_m = K.flatten(y_pred_m)\n","    y_true_m = K.flatten(y_true_m)\n","    mask_prediction_m = K.flatten(mask_prediction_m)\n","\n","\n","    #accuracy   -   precision   -   recall \n","\n","    accuracy = tf.keras.metrics.Accuracy()\n","    accuracy.reset_states()\n","\n","    precision = tf.keras.metrics.Precision()\n","    precision.reset_states()\n","\n","    recall = tf.keras.metrics.Recall()\n","    recall.reset_states()\n","\n","    accuracy.update_state(y_true_m, mask_prediction_m)\n","    precision.update_state(y_true_m, mask_prediction_m)\n","    recall.update_state(y_true_m, mask_prediction_m)\n","\n","    # f1 score\n","\n","    precision_result = precision.result().numpy()\n","    recall_result = recall.result().numpy()\n","    f1_score = 2 * (precision_result * recall_result) / (precision_result + recall_result)\n","\n","    # log-loss\n","\n","    cce = tf.keras.metrics.CategoricalCrossentropy()\n","    cce.reset_states()\n","    cce.update_state(y_true_m1,mask_prediction_m1)\n","\n","    #kld_loss\n","\n","    kld = tf.keras.metrics.KLDivergence()\n","    kld.reset_states()\n","    kld.update_state(y_true_m1, y_pred)\n","    \n","    # poisson_loss\n","\n","    poisson = tf.keras.metrics.Poisson()\n","    poisson.reset_states()\n","    poisson.update_state(y_true_m, y_pred_m)\n","\n","    #dice_loss_with_label\n","\n","    dice_loss = DiceLoss(y_true_m, mask_prediction_m)\n","    \n","    #dice_loss_with_probability \n","\n","    iou_loss =  IoULoss(y_true_m, mask_prediction_m)\n","\n","    return accuracy.result().numpy(), kld.result().numpy(), poisson.result().numpy(), dice_loss, iou_loss, cce.result().numpy(), precision_result, recall_result, f1_score"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-08T09:41:40.733975Z","iopub.status.busy":"2023-05-08T09:41:40.733521Z","iopub.status.idle":"2023-05-08T09:43:42.460664Z","shell.execute_reply":"2023-05-08T09:43:42.459393Z","shell.execute_reply.started":"2023-05-08T09:41:40.733935Z"},"trusted":true},"outputs":[],"source":["for model_name in models_list :\n","    generator = tf.keras.models.load_model(MODELS_PATH + model_name + '.h5', compile=False)\n","\n","    sum_l1_losses = [] \n","    sum_l2_losses = []\n","    sum_psnr_losses = []\n","    sum_ssim_losses = []\n","    sum_acc = []\n","    sum_kld_loss = [] \n","    sum_poisson_loss= []\n","    sum_dice_loss = [] \n","    sum_iou_loss = [] \n","    sum_logloss = []\n","    sum_precision = []\n","    sum_recall = []\n","    sum_f1score = []\n","\n","    for inp, tar in test_dataset :\n","        gen_output = generator(inp, training=False)\n","\n","        acc, kld_loss, poisson_loss, dice_loss, iou_loss, logloss, precision, recall, f1score = segmentation_loss(tar, gen_output)\n","\n","        sum_acc.append(acc)\n","        sum_kld_loss.append(kld_loss)\n","        sum_poisson_loss.append(poisson_loss)\n","        sum_dice_loss.append(dice_loss)\n","        sum_iou_loss.append(iou_loss)\n","        sum_logloss.append(logloss)\n","        sum_precision.append(precision)\n","        sum_recall.append(recall)\n","        sum_f1score.append(f1score)\n","\n","    print(\"\\t  acc \")\n","    print(\"\\t\\t  mean  : \" +  \"{:.2f}\".format(np.mean(sum_acc)) +  \"\\tvar : \" +  \"{:.2f}\".format(np.var(sum_acc)))\n","    print(\"\\t  precision\")\n","    print(\"\\t\\t  mean  : \" +  \"{:.2f}\".format(np.mean(sum_precision)) +  \"\\tvar : \" +  \"{:.2f}\".format(np.var(sum_precision)))\n","    print(\"\\t  recall\")\n","    print(\"\\t\\t  mean  : \" +  \"{:.2f}\".format(np.mean(sum_recall)) +  \"\\tvar : \" +  \"{:.2f}\".format(np.var(sum_recall)))\n","    print(\"\\t  f1 score\")\n","    print(\"\\t\\t  mean  : \" +  \"{:.2f}\".format(np.mean(sum_f1score)) +  \"\\tvar : \" +  \"{:.2f}\".format(np.var(sum_f1score)))\n","    print(\"\\t  cce\")\n","    print(\"\\t\\t  mean  : \" +  \"{:.2f}\".format(np.mean(sum_logloss)) +  \"\\tvar : \" +  \"{:.2f}\".format(np.var(sum_logloss)))\n","    print(\"\\t  poisson loss\")\n","    print(\"\\t\\t  mean  : \" +  \"{:.2f}\".format(np.mean(sum_poisson_loss)) +  \"\\tvar : \" +  \"{:.2f}\".format(np.var(sum_poisson_loss)))\n","    print(\"\\t  dice loss with label\")\n","    print(\"\\t\\t  mean  : \" +  \"{:.2f}\".format(np.mean(sum_dice_loss)) +  \"\\tvar : \" +  \"{:.2f}\".format(np.var(sum_dice_loss)))\n","    print(\"\\t  iou loss with label\")\n","    print(\"\\t\\t  mean  : \" +  \"{:.2f}\".format(np.mean(sum_iou_loss)) +  \"\\tvar : \" +  \"{:.2f}\".format(np.var(sum_iou_loss)))\n","    print(\"\\n\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# label balancing test\n","\n","mean_0 =[]\n","mean_1 =[]\n","mean_2 =[]\n","mean_3 =[]\n","\n","for inp, tar in test_dataset :\n","    label_0 = tar[tar==0]\n","    label_1 = tar[tar==1]\n","    label_2 = tar[tar==2]\n","    label_3 = tar[tar==3]\n","\n","    mean_0.append(label_0.shape[0])\n","    mean_1.append(label_1.shape[0])\n","    mean_2.append(label_2.shape[0])\n","    mean_3.append(label_3.shape[0])\n","\n","print('npixel label 0: ', np.mean(mean_0))\n","print('npixel label 1: ', np.mean(mean_1))\n","print('npixel label 2: ', np.mean(mean_2))\n","print('npixel label 3: ', np.mean(mean_3))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
